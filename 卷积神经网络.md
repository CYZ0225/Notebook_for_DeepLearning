# 卷积神经网络

+ 一种前馈神经网络

+ 受生物学上感受野的机制而提出的

​				在视觉神经系统中,一个神经元的感受野是指视网膜上的特定区域,只有这个区域的刺激才能够激活该神经元.





### 卷积神经网络的结构特性

+ 局部连接
+ 权重共享
+ 空间或时间上的次采样



#### 卷积

+ 卷积经常用在信号处理上,用于计算信息的延迟累积.
+ 假设一个信号发生器每个时刻t产生一个信号$x_t$,其信号的衰减率为$w_k$,即在k-1个时间步长后,信息为原来的$w_k$倍
+ 时刻t收到的信号$y_t$为当前时刻产生的信息和以前时刻延迟信息的叠加.

| 输入信号 | 接受信号(这里假设衰减率为0.5) |
| :------: | :---------------------------: |
|  $x_1$   |           $y_1=x_1$           |
|  $x_2$   |       $y_2=x_2+0.5x_1$        |
|  $x_3$   |   $y_3=x_3+0.5x_2+0.25x_1$    |
|  $x_4$   |           $\cdots$            |
|  $x_5$   |           $\cdots$            |
|  $x_6$   |           $\cdots$            |


$$
\begin{aligned}y_t&=1\times x_t+1/2\times x_{t-1}+\cdots\\&=\sum_{k=1}^Kw_kx_{t-k+1}\end{aligned}
$$
​	这里的$w_k$叫做==滤波器==或者是==卷积核==.

![image-20220615214524544](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220615214524544.png)

如上图所示,下面为输入,上面为卷积输出,其中卷积核要翻转.

> 卷积尺寸公式

$$
H'=\frac{H-K[0]+2Pad[0]}{S[0]}+1
$$

$$
W'=\frac{W-K[1]+2Pad[1]}{S[1]}+1
$$

参数说明:

| 符号  |     含义     |
| :---: | :----------: |
|  H'   | 输出的行大小 |
| $W'$  | 输出的列大小 |
|  $S$  |     步长     |
|  $K$  |  卷积核大小  |
| $Pad$ |  填充的大小  |
|  $H$  | 输入的行大小 |
|  $W$  | 输入的列大小 |

> Note:一般卷积核大小都是奇数.
>

#### 卷积类型

1. 窄卷积:步长$S$=1,两端不补零$P=0$,输出长度为$M-K+1$
2. 宽卷积:步长$S=1$,两端补零$P=K-1$,输出长度为$M+K-1$
3. 等宽卷积:步长$S=1$,两端补零$P=(K-1)/2$,输出长度为$M$

在早期文献中,卷积默认为窄卷积;

在目前文献中,卷积默认为等宽卷积.

### 卷积的作用

1. 近似微分:当卷积核取一些特殊的值,可以做近似微分

2. 低通滤波/高通滤波: 选取不同的滤波器,可以检测信号中的低频信息或高频信息





### 互相关

1. 计算卷积需要进行卷积核翻转
2. 卷积操作的目标:提取特征

​		因此,翻转是不必要的.



于是就有==互相关==.

#### 互相关

$$
y_{ij}=\sum_{u=1}^m\sum_{v=1}^nw_{uv}\cdot x_{i+u-1,j+v-1}
$$

也就是不翻转.

> 一般不声明,我们所讲的卷积就是==互相关==.



#### 卷积层的参数

卷积核的参数大小=$K[0]\times K[1]+1$,其中1来自于偏置项(可以没有)



#### 

#### 多个卷积核

1. 卷积核看作一个特征提取器

   1. 如何增强卷积层能力?

      引入多个卷积核



## 池化层

+ 卷积层虽然可以显著减少连接的个数,但是每一个特征映射的神经元个数并没有显著减少.





## 卷积网络结构

卷积网络是由卷积层,池化层,全连接层交叉堆叠而成.



典型结构:

+ 趋向于小卷积,大深度(即卷积核大小比较小)
+ 趋向于全卷积(就是不池化)



## 其他卷积种类

#### 空洞卷积

如何增加输出单元的感受野?

+ 增加卷积核的大小
+ 增加层数来实现
+ 在卷积之前进行池化操作



空洞卷积:

+ 通过给卷积核插入"空洞"来变相增加其大小.

![image-20220616094949178](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616094949178.png)

#### 转置卷积/微步卷积

低维特征映射到高维特征

![image-20220616095108728](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616095108728.png)

减少卷积核的步数(用小数表示,如0.5步,具体做法是在输入的中间插0实现)





## Inception 模块

在卷积网络中,如何设置卷积层的卷积核大小是一个十分关键的问题.

+ 在Inception网络中,一个卷积核包含多个不同大小的卷积操作,称为Inception模块.
+ Inception模块同时使用$1\times 1,3\times 3,5\times5$等不同大小的卷积核,并将得到的特征映射在深度上拼接(堆叠起来)作为输出特征映射.

![image-20220616100506762](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616100506762.png)



用多层的小卷积核代替大的卷积核,以减少计算量和参数量.

+ 例如,用两层$3\times 3$的卷积核代替$5\times 5$的卷积核
+ 用连续的$n\times 1$和$1\times n$来替换$n \times n$的卷积核.



## 残差网络

残差网络是通过给非线性的卷积层增加==直连边==的方式来提高信息传播效率.

+ 假设在一个深度网络中,我们期望一个非线性单元(可以为一层或多层的卷积层) $f(x;\theta)$去逼近一个目标函数为h(x)
+ 将目标函数拆分成两个部分:恒等函数和残差函数<!--左边为恒等函数,右边为残差函数-->

$$
h(x)=x+(h(x)-x)
$$

其中$h(x)-x为f(x;\theta)$ 

