# 循环神经网络

> 循环神经网络具有记忆性、参数共享(状态转移矩阵$U$,状态输入矩阵$W$,偏置项$b$)、图灵完备

循环神经网络通过使用带自反馈的神经元,能够处理任意长度的时序数据.
$$
h_t=f(h_{t-1},x_t)
$$
![image-20220616104923031](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616104923031.png)

#### 序列到类别

指输入是个序列,输出是个类别

![image-20220616105356703](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616105356703.png)

Case1:情感分类

#### 同步的序列到序列

​	输入的是一个序列,输出的也是一个序列,并且是一一对应的.

![image-20220616105721123](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616105721123.png)

Case1: 中文分词

![image-20220616105956225](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616105956225.png)

图中下面为输入(需要进行word2vec操作),图的上面为类别,其中S表示单个字为词,B表示词的开始,E表示词的结束.



Case2:信息抽取(就是从文字中提取重要信息,即实体识别)

Case3:语音识别

#### 异步的序列到序列

![image-20220616110410588](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220616110410588.png)

Case:机器翻译



### 参数学习

以同步的序列到序列为例



+ 给定一个训练样本$(x,y)$,其中
  + 长度为T的输入序列$x=x_1,x_2,\cdots,x_T$
  + 长度为T的输出序列$y=y_1,y_2,\cdots,y_T$ 



+ 时刻t的瞬间损失函数为

$$
\mathcal{L}_t=\mathcal{L}(y_t,g(h_t))
$$

+ 总损失为

$$
\mathcal{L}=\sum_{i=1}^T\mathcal{L}_t
$$

### 计算梯度

把原来的式子写成两个式子,方便对照前馈神经网络
$$
z_t=Uh_{t-1}+Wx_t+b\\
h_t=f(z_t)
$$
梯度:
$$
\begin{aligned}
\frac{\part \mathcal{L}}{\part U} &=\sum_{t=1}^T \frac{\part \mathcal{L}_t}{\part U}\\
\end{aligned}
$$
其中,==这里的U表示的是总体,如果对应一个位置的话==
$$
\begin{aligned}
\frac{\part\mathcal{L}_t}{\part U}&=\sum_{k=1}^t\frac{\part \mathcal{L}_t}{\part z_k}h_{k-1}^T \\
\end{aligned}
$$
对于中间的$\frac{\part \mathcal{L}_t}{\part z_k}$有:
$$
\begin{aligned}
令\delta_{t,k}&=\frac{\part \mathcal{L}_t}{\part z_k}\\
&=\frac{\part h_k}{\part z_k}\frac{\part z_{k+1}}{\part h_k}\frac{\part \mathcal{L}_t}{\part z_{k+1}}\\
&=diag(f'(z_k))U^T\delta_{t,k+1}
\end{aligned}
$$
上式就是BPTT(BackPropagation Through Time)算法.

> Note:==长程依赖问题==
>
> 由于梯度爆炸或梯度消失问题,实际上只能学到短周期的依赖关系

由上面的式子可以得到:
$$
\begin{aligned}
\delta_{t,k}&=\prod_{\tau=k}^{t-1}(diag(f'(z_\tau))U^T)\delta_{t,t}\\
\end{aligned}
$$
于是有:
$$
\delta_{t,k}\cong\gamma^{t-k}\delta_{t,t}=\begin{cases}
{\infty \qquad if \quad\gamma\leq1\qquad }\\{0\qquad if \quad\gamma\geq1\qquad }\end{cases}
$$
即出现长程依赖问题.





### 如何解决长程依赖问题?

==循环神经网络在时间维度上非常深!==

+ 梯度消失或梯度爆炸



==如何改进?==

+ 梯度爆炸问题
  + 权重衰减
  + 梯度截断



+ 梯度消失问题
  + 改进模型



### 门控机制

控制信息的累计速度,包括有选择地加入新的信息,并有选择地遗忘之前累计的信息.

+ 门控循环单元(Gate Recurrent Unit,GRU)
+ 长短期记忆网络(Long Short-Term Memory,LSTM)

#### GRU

$$
h_t=\color{red}z_t\color{black}\odot h_{t-1}+\color{red}(1-z_t)\color{black}\odot g(x,h_{t-1};\theta)
$$

其中$z_t \in (0,1)^d$ 

$z_t=\sigma(W_zx_t+U_zh_{t-1}+b_z)$称为**更新门**,因为logistics函数限制在(0,1)
$$
g(x,h_{t-1};\theta)=\tanh(W_hx_t+U_h\color{red}r_t\color{black}(h_{t-1})+b_h)
$$
其中$r_t$称为重置门,当$z_t=0$且$r_t=0$,则输出与前一项无任何关系.

![image-20220617000614852](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220617000614852.png)

#### LSTM

$$
\widetilde{c_t}=\tanh(W_cx_t+U_ch_{t-1}+b_c)\\
c_t=f_t\odot c_{t-1}+i_t\odot\widetilde{c_t}\\
h_t=o_t\odot\tanh(c_t)
$$

其中$i_t$叫做记忆门,$f_t$叫做遗忘门,$o_t$叫做输出门
$$
i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)\\
f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)\\
o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)
$$


![](C:\Users\一个路过的程序员\AppData\Roaming\Typora\typora-user-images\image-20220617100123615.png)

### LSTM的各种变体形式

#### 没有遗忘门

$$
c_t=c_{t-1}+i_t\odot\widetilde{c_t}
$$

#### 耦合输入门和遗忘门

$$
f_t=1-i_t\\
c_t=(1-i_t)\odot c_{t-1}+i_t\odot\widetilde{c_t}
$$

#### peephole连接

$$
i_t=\sigma(W_ix_i+U_ih_{t-1}+V_ic_{t-1}+b_i)\\
f_t=\sigma(W_fx_i+U_fh_{t-1}+V_fc_{t-1}+b_f)\\
o_t=\sigma(W_ox_t+U_oh_{t-1}+V_oc_t+b_o)
$$

### 深层循环神经网络

1. 堆叠循环神经网络(就是num_layers不为1)
2.  双向循环神经网络





### 小结

|      优点      |     缺点     |
| :------------: | :----------: |
| 引入(短期)记忆 | 长程依赖问题 |
|    图灵完备    | 记忆容量问题 |
|                |   并行能力   |

